# -*- coding: utf-8 -*-
"""Day4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yzl1LmtTlxRpAuqMqLI6OE1JBuUPeCye

# Linear Regression
Linear Regression ia a method to find a stright-line relationship between two thing - one you already know and one you want to predict

---Prediction formula = mx+c

---slope m =n(∑(xy))-(∑(x))*(∑(y))/n(∑(x^2))-(∑(x)^2)

---intercept c=(ȳ)-m.(x̄)
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

hours = [1,2,3,4,5]

marks = [35,45,50,60,75]

x=np.array(hours).reshape(-1,1)
print(x)
y=np.array(marks)
print(y)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

hours = [1,2,3,4,5]

marks = [35,45,50,60,75]

x=np.array(hours).reshape(-1,1)
y=np.array(marks)

model = LinearRegression()
model.fit(x,y)

predicted = model.predict([[6]])
print(model.predict([[6]]))
print("Predicted Marks for 6 hours of study:",predicted[0])

plt.scatter(x,y,color='blue',label='Actual Data')
plt.plot(x,model.predict(x),color='red',label='Best Fit Line')
plt.xlabel('Hours of Study')
plt.ylabel('Marks')
plt.title('Study Hours vs Marks')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error,r2_score

data = pd.read_csv("people_data.csv")

data = data[['Age','Salary']]

imputer = SimpleImputer(strategy='mean')
data_filled = pd.DataFrame(imputer.fit_transform(data),columns = ['Age', 'Salary'])

x = data_filled[['Age']]
y = data_filled['Salary']

model = LinearRegression()
model.fit(x,y)

predicted_salary = model.predict([[32]])
print("Predicted Salary for age 32:",predicted_salary[0])

y_pred = model.predict(x)

mse = mean_squared_error(y,y_pred)
r2 = r2_score(y,y_pred)

print("Mean Squared Error:",mse)
print("R-squared:",r2)

plt.scatter(x,y,color='blue',label='Actual Data')
plt.plot(x,model.predict(x),color='red',label='Predicted Line')
plt.xlabel('Age')
plt.ylabel('Salary')
plt.title('Age vs Salary')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import mean_squared_error,r2_score

actual = [3,5,7,9]
predicted = [2.5,4.8,7.2,8.5]

mse = mean_squared_error(actual,predicted)
r2 = r2_score(actual,predicted)

print(f"Mean Squared Error: {mse:.3f}")
print(f"R-squared: {r2:.3f}")

var = input()
empty_str =str()
for i in var:
    empty_str = i+empty_str
print(empty_str)

if var == empty_str:
    print("Palindrome")
else:
    print("Not Palindrome")

"""# Polinomial Regression
Polinomial features are new features created by taking the original features and raising them to powers (like x^2,x^3,etc) including combinations if there are multiple features
"""

X=[[2],[3],[4]]
from sklearn.preprocessing import PolynomialFeatures
ploy = PolynomialFeatures(degree=2)
X_poly = ploy.fit_transform(X)
print(X_poly)

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_squared_error, r2_score

#sample data: YearsExperiance vs Salary
data=pd.DataFrame({
    "YearsExperiance": [1,2,3,4,5,6,7,8,9,10],
    "Salary": [30000,35000,40000,50000,60000,80000,110000,150000,200000,300000]
})
x=data[["YearsExperiance"]]
y=data["Salary"]

#train linear model
lin_reg=LinearRegression()
lin_reg.fit(x,y)

#predict
y_pred_linear=lin_reg.predict(x)

#convert to polynomial features (degree 2 orb 3 usually works well)
poly=PolynomialFeatures(degree=5)
x_poly=poly.fit_transform(x)
print(x_poly)

#train polynomial model
poly_reg=LinearRegression()
poly_reg.fit(x_poly,y)

#predict
y_pred_poly=poly_reg.predict(x_poly)

#plot the data and the prediction line
plt.scatter(x,y,color="green",label="Actual Data") #actual data
plt.plot(x,y_pred_linear,color="blue",label="LinearRegression")
plt.plot(x,y_pred_poly,color="red",label="PolnomialRegression(Degree 2)")
plt.xlabel("Years of Experiance")
plt.ylabel("Salary")
plt.title("Salary Prediction: Linear vs Polynomial")
plt.legend()

plt.show()

print("Linear R2:", r2_score(y, y_pred_linear))
print("Polynomial R2:", r2_score(y, y_pred_poly))

"""Importing Libraries"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

"""Imorting dataset"""

dataset = pd.read_csv("Position_Salaries.csv")
print(dataset)
x= dataset.iloc[:,1:-1].values
y= dataset.iloc[:,-1].values

"""Training the Linear Regression model on the whole dataset"""

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(x,y)

"""Training the Polinomial Regression model on the whole dataset"""

from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=4)
x_poly = poly_reg.fit_transform(x)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(x_poly,y)

"""Visualisng the Linear Regression results"""

plt.scatter(x,y,color = 'red')
plt.plot(x,lin_reg.predict(x),color = 'blue')
plt.title('Truth or Bluff(Linear Regression)')
plt.xlabel('Position Level')
plt.ylabel('Salary')
plt.show()

"""Visualisng the Linear Regression results"""

plt.scatter(x,y,color = 'red')
plt.plot(x,lin_reg_2.predict(poly_reg.fit_transform(x)),color = 'blue')
plt.title('Truth or Bluff(Polinomial Regression)')
plt.xlabel('Position Level')
plt.ylabel('Salary')
plt.show()

"""## Predicting a new result with Linear Regression"""

lin_reg.predict([[6.5]])

"""## Predicting a new result with Polynomial Regression"""

lin_reg_2.predict(poly_reg.fit_transform([[6.5]]))